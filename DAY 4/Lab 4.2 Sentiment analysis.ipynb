{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "163cdc86",
   "metadata": {},
   "source": [
    "**Simple Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dba7b8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4/4 [==============================] - 1s 88ms/step - loss: 0.7009 - accuracy: 0.3750 - val_loss: 0.6913 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.6892 - accuracy: 0.6250 - val_loss: 0.6848 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.6766 - accuracy: 0.8750 - val_loss: 0.6872 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.6635 - accuracy: 1.0000 - val_loss: 0.6968 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.6452 - accuracy: 1.0000 - val_loss: 0.7019 - val_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.7019 - accuracy: 0.0000e+00\n",
      "Accuracy: 0.0\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "Raw predictions: [[0.51003903]\n",
      " [0.49818763]]\n",
      "Review: \"I loved the movie, it was fantastic!\" - Sentiment: positive\n",
      "Review: \"The movie was terrible and boring.\" - Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a simple dataset\n",
    "data = {\n",
    "    'review': [\n",
    "        'I love this movie, it is fantastic!',\n",
    "        'This film was terrible and boring.',\n",
    "        'Absolutely wonderful and a great experience.',\n",
    "        'I did not like the movie at all.',\n",
    "        'The plot was dull and uninteresting.',\n",
    "        'Brilliant performance by the actors.',\n",
    "        'The movie was so bad, I walked out.',\n",
    "        'One of the best movies I have ever seen.',\n",
    "        'It was an okay movie, nothing special.',\n",
    "        'Horrible, I would not recommend it to anyone.'\n",
    "    ],\n",
    "    'sentiment': [\n",
    "        'positive', 'negative', 'positive', 'negative', 'negative',\n",
    "        'positive', 'negative', 'positive', 'neutral', 'negative'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Encode the sentiment labels\n",
    "df['sentiment'] = df['sentiment'].replace({'positive': 1, 'negative': 0, 'neutral': 0})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('<br />', ' ', text)\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    return text\n",
    "\n",
    "df['review'] = df['review'].apply(preprocess_text)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df['review'])\n",
    "X = tokenizer.texts_to_sequences(df['review'])\n",
    "X = pad_sequences(X, maxlen=20)\n",
    "\n",
    "# Encode labels\n",
    "y = df['sentiment'].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=2000, output_dim=128, input_length=20))\n",
    "model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=2, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Predict sentiment for new reviews\n",
    "new_reviews = [\"I loved the movie, it was fantastic!\", \"The movie was terrible and boring.\"]\n",
    "new_reviews_preprocessed = [preprocess_text(review) for review in new_reviews]\n",
    "new_reviews_sequences = tokenizer.texts_to_sequences(new_reviews_preprocessed)\n",
    "new_reviews_padded = pad_sequences(new_reviews_sequences, maxlen=20)\n",
    "predictions = model.predict(new_reviews_padded)\n",
    "\n",
    "# Print raw predictions to debug\n",
    "print(\"Raw predictions:\", predictions)\n",
    "\n",
    "# Convert predictions to \"positive\" or \"negative\"\n",
    "predicted_labels = ['positive' if pred >= 0.5 else 'negative' for pred in predictions]\n",
    "\n",
    "for review, sentiment in zip(new_reviews, predicted_labels):\n",
    "    print(f'Review: \"{review}\" - Sentiment: {sentiment}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eeadd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a simple dataset\n",
    "\n",
    "# Load the dataset\n",
    "imdb = pd.read_csv('./imdb.csv', sep='\\t',encoding='latin-1')\n",
    "df = imdb.head(15000).copy()\n",
    "\n",
    "# Encode the sentiment labels\n",
    "df['sentiment'] = df['sentiment'].replace({'positive': 1, 'negative': 0})\n",
    "# Drop the 'id' column from the DataFrame\n",
    "df.drop('id', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e3196640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review  sentiment\n",
      "0      With all this stuff going down at the moment w...          1\n",
      "1      \\The Classic War of the Worlds\\\" by Timothy Hi...          1\n",
      "2      The film starts with a manager (Nicholas Bell)...          0\n",
      "3      It must be assumed that those who praised this...          0\n",
      "4      Superbly trashy and wondrously unpretentious 8...          1\n",
      "...                                                  ...        ...\n",
      "14995  The Last Station, director Michael Hoffman's m...          0\n",
      "14996  Silly, often ridiculous romp involving the lan...          0\n",
      "14997  Was this the greatest movie that I have ever s...          0\n",
      "14998  We've all seen this story a hundred times. You...          0\n",
      "14999  This crime thriller is sort of like a film noi...          1\n",
      "\n",
      "[15000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e0415d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('<br />', ' ', text)\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    return text\n",
    "\n",
    "df['review'] = df['review'].apply(preprocess_text)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df['review'])\n",
    "X = tokenizer.texts_to_sequences(df['review'])\n",
    "X = pad_sequences(X, maxlen=20)\n",
    "\n",
    "# Encode labels\n",
    "y = df['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e977c89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 82s 28ms/step - loss: 0.5710 - accuracy: 0.6940 - val_loss: 0.5156 - val_accuracy: 0.7472\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 114s 41ms/step - loss: 0.4239 - accuracy: 0.8006 - val_loss: 0.5539 - val_accuracy: 0.7419\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 138s 49ms/step - loss: 0.3209 - accuracy: 0.8596 - val_loss: 0.6208 - val_accuracy: 0.7344\n",
      "118/118 [==============================] - 2s 17ms/step - loss: 0.6208 - accuracy: 0.7344\n",
      "Accuracy: 0.7343999743461609\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Build the LSTM model\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=128, input_length=20))  # Adjust input_dim and input_length accordingly\n",
    "model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=3, batch_size=4, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5b8c2e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 285ms/step\n",
      "Raw predictions: [[0.02149437]\n",
      " [0.9439366 ]\n",
      " [0.00379151]]\n",
      "Review: \"Well done Al Gore!\" - Sentiment: negative\n",
      "Review: \"I loved the movie, it was fantastic!\" - Sentiment: positive\n",
      "Review: \"terrible and boring.\" - Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "# Predict sentiment for new reviews\n",
    "new_reviews = [ \"Well done Al Gore!\",\"I loved the movie, it was fantastic!\", \"terrible and boring.\"]\n",
    "new_reviews_preprocessed = [preprocess_text(review) for review in new_reviews]\n",
    "new_reviews_sequences = tokenizer.texts_to_sequences(new_reviews_preprocessed)\n",
    "new_reviews_padded = pad_sequences(new_reviews_sequences, maxlen=20)\n",
    "predictions = model.predict(new_reviews_padded)\n",
    "\n",
    "# Print raw predictions to debug\n",
    "print(\"Raw predictions:\", predictions)\n",
    "\n",
    "# Convert predictions to \"positive\" or \"negative\"\n",
    "predicted_labels = ['positive' if pred >= 0.5 else 'negative' for pred in predictions]\n",
    "\n",
    "for review, sentiment in zip(new_reviews, predicted_labels):\n",
    "    print(f'Review: \"{review}\" - Sentiment: {sentiment}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa335ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7d608e1",
   "metadata": {},
   "source": [
    "**Sentiment Analysis using Hugging Face Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5c9fd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformersNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "     ---------------------------------------- 9.3/9.3 MB 10.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\acer\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "     ------------------------------------- 402.6/402.6 kB 12.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in c:\\users\\acer\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp39-none-win_amd64.whl (287 kB)\n",
      "     -------------------------------------- 287.9/287.9 kB 8.7 MB/s eta 0:00:00\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp39-none-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 10.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "     ------------------------------------- 177.6/177.6 kB 10.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\acer\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.2.0\n",
      "    Uninstalling fsspec-2022.2.0:\n",
      "      Successfully uninstalled fsspec-2022.2.0\n",
      "Successfully installed fsspec-2024.6.1 huggingface-hub-0.23.4 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.42.3\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6740ef9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\acer\\anaconda3\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torch) (4.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\acer\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "776a8162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\acer\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8973fd95fb462a9678395a05b904a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\acer\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70bff7378f4d4df9ba67755c40aef6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6faf93120c8d40ac9c23bfdafedb6dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7489663224164676bc2f4bac331f1b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this product! It works great.\n",
      "Sentiment: POSITIVE, Score: 0.9998793601989746\n",
      "\n",
      "Text: This is the worst service I have ever received.\n",
      "Sentiment: NEGATIVE, Score: 0.9997833371162415\n",
      "\n",
      "Text: It's an average experience, nothing special.\n",
      "Sentiment: NEGATIVE, Score: 0.9993513226509094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the sentiment-analysis pipeline\n",
    "sentiment_pipeline = pipeline('sentiment-analysis')\n",
    "\n",
    "# Example texts\n",
    "texts = [\n",
    "    \"I love this product! It works great.\",\n",
    "    \"This is the worst service I have ever received.\",\n",
    "    \"It's an average experience, nothing special.\"\n",
    "]\n",
    "\n",
    "# Perform sentiment analysis\n",
    "results = sentiment_pipeline(texts)\n",
    "\n",
    "# Print the results\n",
    "for text, result in zip(texts, results):\n",
    "    print(f\"Text: {text}\\nSentiment: {result['label']}, Score: {result['score']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59fbf92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
